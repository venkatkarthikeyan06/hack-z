<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Face Recognition and Eye Movement Detection</title>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/blazeface"></script>
    <script async src="https://docs.opencv.org/4.x/opencv.js" type="text/javascript" onload="onOpenCvReady()"></script>
    <style>
        body {
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            height: 100vh;
            background-color: #f5f7fa;
        }
        video {
            display: none; /* Hide video element */
        }
        canvas {
            position: absolute;
        }
    </style>
</head>
<body>
    <h1>Face Recognition and Eye Movement Detection</h1>
    <video id="video" width="720" height="560" autoplay muted></video>
    <canvas id="canvas" width="720" height="560"></canvas>
    <p id="message"></p>

    <script>
        // Function called when OpenCV.js is ready
        function onOpenCvReady() {
            console.log("OpenCV.js is ready!");
            detectFace();
        }

        async function setupCamera() {
            const video = document.getElementById('video');
            const stream = await navigator.mediaDevices.getUserMedia({ video: true });
            video.srcObject = stream;

            return new Promise((resolve) => {
                video.onloadedmetadata = () => {
                    resolve(video);
                };
            });
        }

        async function detectFace() {
            const video = await setupCamera();
            video.play();

            const model = await blazeface.load();  // Correct function to load BlazeFace model
            const canvas = document.getElementById('canvas');
            const ctx = canvas.getContext('2d');

            async function detect() {
                const predictions = await model.estimateFaces(video, false);
                ctx.clearRect(0, 0, canvas.width, canvas.height);
                ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

                predictions.forEach(prediction => {
                    ctx.beginPath();
                    ctx.rect(
                        prediction.topLeft[0],
                        prediction.topLeft[1],
                        prediction.bottomRight[0] - prediction.topLeft[0],
                        prediction.bottomRight[1] - prediction.topLeft[1]
                    );
                    ctx.lineWidth = 2;
                    ctx.strokeStyle = 'red';
                    ctx.stroke();
                });

                if (predictions.length > 0) {
                    // Call the function to check eye movement
                    checkEyeMovement(video);
                }

                requestAnimationFrame(detect);
            }

            detect();
        }

        async function checkEyeMovement(video) {
            if (video.readyState === 4) { // Ensure the video is ready
                // Capture video frame
                const canvas = document.createElement('canvas');
                canvas.width = video.videoWidth;
                canvas.height = video.videoHeight;
                const ctx = canvas.getContext('2d');
                ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
                const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);

                // Manually create a Mat object from ImageData
                const src = cv.matFromArray(imageData.height, imageData.width, cv.CV_8UC4, imageData.data);
                const gray = new cv.Mat();
                cv.cvtColor(src, gray, cv.COLOR_RGBA2GRAY);

                // Load Haar Cascade for eye and face detection
                const eyeCascade = new cv.CascadeClassifier();
                const faceCascade = new cv.CascadeClassifier();

                eyeCascade.load('haarcascade_eye.xml');  // Ensure you have this file available
                faceCascade.load('haarcascade_frontalface_default.xml'); // Ensure this file is available

                const eyes = new cv.RectVector();
                const faces = new cv.RectVector();

                faceCascade.detectMultiScale(gray, faces, 1.1, 3, 0);

                for (let i = 0; i < faces.size(); i++) {
                    const face = faces.get(i);
                    const roiGray = gray.roi(face);
                    eyeCascade.detectMultiScale(roiGray, eyes, 1.1, 3, 0);

                    if (eyes.size() > 0) {
                        document.getElementById('message').textContent = "Eyes Detected!";
                    } else {
                        document.getElementById('message').textContent = "Eyes Closed!";
                    }
                    roiGray.delete();
                }

                // Clean up
                src.delete();
                gray.delete();
                eyes.delete();
                faces.delete();
            } else {
                console.warn("Video not ready for processing.");
            }
        }

        
    </script>
</body>
</html>
